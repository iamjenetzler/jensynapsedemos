{
	"name": "Retail 02 -Sales Forecasting",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b9913441-8a8e-4fb1-812a-9fcbe1dd9aba/resourceGroups/Synapse580/providers/Microsoft.Synapse/workspaces/synapse580/bigDataPools/spark1",
				"name": "spark1",
				"type": "Spark",
				"endpoint": "https://synapse580.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Near real-time sales forecasting leveraging Synapse Link for Azure Cosmos DB\n",
					"\n",
					"Predictive analytics can help us to study and discover the factors that determine the number of sales that a retail store will have in the future.\n",
					"\n",
					"This notebook scenario is [Microsoft Surface](https://www.microsoft.com/en-us/surface) sales forecasting, with artificially created data. The business challenge is a **distributor that wants to predict how many units are necessary in the local warehouse to supply the stores in the area.**\n",
					"\n",
					"We will use Quantitative Models to forecast future data as a function of past data. They are appropriate to use when past numerical data is available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. These methods are usually applied to short or intermediate range decisions. For more information, click [here](https://en.wikipedia.org/wiki/Forecasting).\n",
					"\n",
					"\n",
					"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/store.PNG\" alt=\"Surface Device\" width=\"75%\"/>"
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Environment Creation\n",
					"\n",
					"This is a Synapse Notebook and it runs without any customization in Synapse Analytics workspaces. Using Azure Machine Learning SDK, it will run an AutomatedML process within Azure Synapse Spark Pool, reading data from Azure Cosmos Db Analytical Store.\n",
					"\n",
					"The required steps are listed below. Please check the availabilty and try to colocate all these services in the same Azure region. Also, if you already have any of these services, created in another sample notebook, it is not required to create new ones.\n",
					"\n",
					"1. [README](/README.md) file pre-reqs.\n",
					"1. Load the data using the [Batch Ingestion notebook](1CosmoDBSynapseSparkBatchIngestion.ipynb).\n",
					"1. Create an Azure Machine Learning workspace using [this](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace) tutorial.\n",
					"1. Create an Azure Automated Machine Learning experiment using [this](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-automated-ml-for-ml-models) tutorial. You don't need to run it, neither to create a compute cluster.\n",
					"1. Go to your **Synapse Spark Pool**, click on the **Packages Settings** tab, upload the [req.txt](req.txt) file located in this same folder, and click **force to get new settings on the spark pool** option. "
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Leverage power of Spark SQL to join & aggregate operational data across Cosmos DB containers"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [],
							"values": [],
							"yLabel": "",
							"xLabel": "",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": true
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"create database if not exists RetailSalesDemoDB"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [],
							"values": [],
							"yLabel": "",
							"xLabel": "",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": true
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"create table if not exists RetailSalesDemoDB.RetailSales using cosmos.olap options (\n",
					"    spark.synapse.linkedService 'RetailSalesDemoDB',\n",
					"    spark.cosmos.preferredRegions 'West US 2',\n",
					"    spark.cosmos.container 'RetailSales'\n",
					")"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [],
							"values": [],
							"yLabel": "",
							"xLabel": "",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": true
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"create table if not exists RetailSalesDemoDB.StoreDemographics using cosmos.olap options (\n",
					"    spark.synapse.linkedService 'RetailSalesDemoDB',\n",
					"    spark.cosmos.preferredRegions 'West US 2',\n",
					"    spark.cosmos.container 'StoreDemoGraphics'\n",
					")"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "sparksql"
					},
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [],
							"values": [],
							"yLabel": "",
							"xLabel": "",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": true
					},
					"collapsed": false
				},
				"source": [
					"%%sql\n",
					"create table if not exists RetailSalesDemoDB.Product using cosmos.olap options (\n",
					"    spark.synapse.linkedService 'RetailSalesDemoDB',\n",
					"    spark.cosmos.preferredRegions 'West US 2',\n",
					"    spark.cosmos.container 'Products'\n",
					")"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"storeId"
							],
							"values": [
								"storeId"
							],
							"yLabel": "storeId",
							"xLabel": "storeId",
							"aggregation": "COUNT",
							"aggByBackend": false
						},
						"aggData": "{\"storeId\":{\"2\":12,\"5\":12,\"8\":12,\"9\":12,\"12\":12,\"14\":12,\"18\":12,\"21\":12,\"28\":12,\"32\":12,\"33\":12,\"40\":12,\"44\":9,\"45\":12,\"47\":12,\"48\":12,\"49\":12,\"50\":12,\"51\":12,\"52\":12,\"53\":12,\"54\":12,\"56\":12,\"59\":12,\"62\":12,\"64\":12,\"67\":12,\"68\":9,\"70\":12,\"71\":12,\"72\":12,\"73\":12,\"74\":12,\"75\":12,\"76\":12,\"77\":12,\"78\":12,\"80\":12,\"81\":12,\"83\":12,\"84\":12,\"86\":12,\"88\":12,\"89\":12,\"90\":12,\"91\":12,\"92\":12,\"93\":9,\"94\":12,\"95\":12,\"97\":12,\"98\":12,\"100\":15,\"101\":15,\"102\":15,\"103\":15,\"104\":12,\"105\":15,\"106\":15,\"107\":14,\"109\":12,\"110\":12,\"111\":12,\"112\":12,\"113\":12,\"114\":12,\"115\":12,\"116\":12,\"117\":12,\"118\":12,\"119\":12,\"121\":12,\"122\":12,\"123\":12,\"124\":12,\"126\":12,\"128\":12,\"129\":9,\"130\":12,\"131\":12,\"132\":12,\"134\":9,\"137\":12}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"data = spark.sql(\"select a.storeId \\\n",
					"                       , b.productCode \\\n",
					"                       , b.wholeSaleCost \\\n",
					"                       , b.basePrice \\\n",
					"                       , c.ratioAge60 \\\n",
					"                       , c.collegeRatio \\\n",
					"                       , c.income \\\n",
					"                       , c.highIncome150Ratio \\\n",
					"                       , c.largeHH \\\n",
					"                       , c.minoritiesRatio \\\n",
					"                       , c.more1FullTimeEmployeeRatio \\\n",
					"                       , c.distanceNearestWarehouse \\\n",
					"                       , c.salesNearestWarehousesRatio \\\n",
					"                       , c.avgDistanceNearest5Supermarkets \\\n",
					"                       , c.salesNearest5StoresRatio \\\n",
					"                       , a.quantity \\\n",
					"                       , a.logQuantity \\\n",
					"                       , a.advertising \\\n",
					"                       , a.price \\\n",
					"                       , a.weekStarting \\\n",
					"                 from RetailSalesDemoDB.retailsales a \\\n",
					"                 left join RetailSalesDemoDB.product b \\\n",
					"                 on a.productcode = b.productcode \\\n",
					"                 left join RetailSalesDemoDB.storedemographics c \\\n",
					"                 on a.storeId = c.storeId \\\n",
					"                 order by a.weekStarting, a.storeId, b.productCode\")\n",
					"\n",
					"display(data)"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Leverage power of Azure Machine Learning's AutoML to build a Forecasting Model\n",
					"\n",
					"### Setup \n",
					"Let's start with the AML Environment setup. Please replace all \"your-AML-%\" placeholders with your own information, that you can get from your AML workspace in the the Azure Portal.\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"import azureml.core\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"import logging\n",
					"from azureml.core.workspace import Workspace\n",
					"from azureml.core import Workspace\n",
					"from azureml.core.experiment import Experiment\n",
					"from azureml.train.automl import AutoMLConfig\n",
					"import os\n",
					"subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"b9913441-8a8e-4fb1-812a-9fcbe1dd9aba\")\n",
					"resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"synapse580\")\n",
					"workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"machinelearning580ws\")\n",
					"workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"West US 2\")\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"ws.write_config()\n",
					"    \n",
					"experiment_name = 'automl-surfaceforecasting'\n",
					"experiment = Experiment(ws, experiment_name)\n",
					"output = {}\n",
					"output['Subscription ID'] = ws.subscription_id\n",
					"output['Workspace'] = ws.name\n",
					"output['SKU'] = ws.sku\n",
					"output['Resource Group'] = ws.resource_group\n",
					"output['Location'] = ws.location\n",
					"output['Run History Name'] = experiment_name\n",
					"pd.set_option('display.max_colwidth', -1)\n",
					"outputDf = pd.DataFrame(data = output, index = [''])\n",
					"outputDf.T"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Data Preparation - Feature engineering, Splitting train & test datasets\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Initial variables\n",
					"time_column_name = 'weekStarting'\n",
					"grain_column_names = ['storeId', 'productCode']\n",
					"target_column_name = 'quantity'\n",
					"use_stores = [2, 5, 8,71,102]\n",
					"n_test_periods = 20\n",
					"\n",
					"\n",
					"#DataFrame\n",
					"df = data.toPandas()\n",
					"df[time_column_name] = pd.to_datetime(df[time_column_name])\n",
					"df['storeId'] = pd.to_numeric(df['storeId'])\n",
					"df['quantity'] = pd.to_numeric(df['quantity'])\n",
					"df['advertising'] = pd.to_numeric(df['advertising'])\n",
					"df['price'] = df['price'].astype(float)\n",
					"df['basePrice'] = df['basePrice'].astype(float)\n",
					"df['ratioAge60'] = df['ratioAge60'].astype(float)\n",
					"df['collegeRatio'] = df['collegeRatio'].astype(float)\n",
					"df['highIncome150Ratio'] = df['highIncome150Ratio'].astype(float)\n",
					"df['income'] = df['income'].astype(float)\n",
					"df['largeHH'] = df['largeHH'].astype(float)\n",
					"df['minoritiesRatio'] = df['minoritiesRatio'].astype(float)\n",
					"df['logQuantity'] = df['logQuantity'].astype(float)\n",
					"df['more1FullTimeEmployeeRatio'] = df['more1FullTimeEmployeeRatio'].astype(float)\n",
					"df['distanceNearestWarehouse'] = df['distanceNearestWarehouse'].astype(float)\n",
					"df['salesNearestWarehousesRatio'] = df['salesNearestWarehousesRatio'].astype(float)\n",
					"df['avgDistanceNearest5Supermarkets'] = df['avgDistanceNearest5Supermarkets'].astype(float)\n",
					"df['salesNearest5StoresRatio'] = df['salesNearest5StoresRatio'].astype(float)\n",
					"\n",
					"\n",
					"# Time Series\n",
					"data_subset = df[df.storeId.isin(use_stores)]\n",
					"nseries = data_subset.groupby(grain_column_names).ngroups\n",
					"print('Data subset contains {0} individual time-series.'.format(nseries))\n",
					"\n",
					"# Group by date\n",
					"def split_last_n_by_grain(df, n):\n",
					"    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
					"    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
					"                  .groupby(grain_column_names, group_keys=False))\n",
					"    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
					"    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
					"    return df_head, df_tail\n",
					"\n",
					"# splitting\n",
					"train, test = split_last_n_by_grain(data_subset, n_test_periods)\n",
					"print(len(train),len(test))\n",
					"train.to_csv (r'./SurfaceSales_train.csv', index = None, header=True)\n",
					"test.to_csv (r'./SurfaceSales_test.csv', index = None, header=True)\n",
					"datastore = ws.get_default_datastore()\n",
					"datastore.upload_files(files = ['./SurfaceSales_train.csv', './SurfaceSales_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)\n",
					"\n",
					"# loading the train dataset\n",
					"from azureml.core.dataset import Dataset\n",
					"train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/SurfaceSales_train.csv'))"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Training the Models using AutoML Forecasting\n",
					"\n",
					"Please notice that **compute_target** is commented, meaning that the model training will run locally in Synapse Spark."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Parameters\n",
					"time_series_settings = {\n",
					"    'time_column_name': time_column_name,\n",
					"    'grain_column_names': grain_column_names,\n",
					"    'max_horizon': n_test_periods\n",
					"}\n",
					"\n",
					"# Config\n",
					"automl_config = AutoMLConfig(task='forecasting',\n",
					"                             debug_log='automl_ss_sales_errors.log',\n",
					"                             primary_metric='normalized_mean_absolute_error',\n",
					"                             experiment_timeout_hours=0.5,\n",
					"                             training_data=train_dataset,\n",
					"                             label_column_name=target_column_name,\n",
					"                             #compute_target=compute_target,\n",
					"                             enable_early_stopping=True,\n",
					"                             n_cross_validations=3,\n",
					"                             verbosity=logging.INFO,\n",
					"                             **time_series_settings)\n",
					"\n",
					"# Running the training\n",
					"remote_run = experiment.submit(automl_config, show_output=True)\n",
					"\n",
					""
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Retrieving the Best Model and Forecasting"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Retrieving the best model\n",
					"best_run, fitted_model = remote_run.get_output()\n",
					"print(fitted_model.steps)\n",
					"model_name = best_run.properties['model_name']\n",
					"print(model_name)\n",
					"\n",
					"# Forecasting based on test dataset\n",
					"X_test = test\n",
					"y_test = X_test.pop(target_column_name).values\n",
					"X_test[time_column_name] = pd.to_datetime(X_test[time_column_name])\n",
					"y_predictions, X_trans = fitted_model.forecast(X_test)"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Plotting the Results"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"import numpy as np\n",
					"from pandas.tseries.frequencies import to_offset\n",
					"\n",
					"\n",
					"def align_outputs(y_predicted, X_trans, X_test, y_test, target_column_name,\n",
					"                  predicted_column_name='predicted',\n",
					"                  horizon_colname='horizon_origin'):\n",
					"    \"\"\"\n",
					"    Demonstrates how to get the output aligned to the inputs\n",
					"    using pandas indexes. Helps understand what happened if\n",
					"    the output's shape differs from the input shape, or if\n",
					"    the data got re-sorted by time and grain during forecasting.\n",
					"\n",
					"    Typical causes of misalignment are:\n",
					"    * we predicted some periods that were missing in actuals -> drop from eval\n",
					"    * model was asked to predict past max_horizon -> increase max horizon\n",
					"    * data at start of X_test was needed for lags -> provide previous periods\n",
					"    \"\"\"\n",
					"\n",
					"    if (horizon_colname in X_trans):\n",
					"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n",
					"                                horizon_colname: X_trans[horizon_colname]})\n",
					"    else:\n",
					"        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n",
					"\n",
					"    # y and X outputs are aligned by forecast() function contract\n",
					"    df_fcst.index = X_trans.index\n",
					"\n",
					"    # align original X_test to y_test\n",
					"    X_test_full = X_test.copy()\n",
					"    X_test_full[target_column_name] = y_test\n",
					"\n",
					"    # X_test_full's index does not include origin, so reset for merge\n",
					"    df_fcst.reset_index(inplace=True)\n",
					"    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
					"    together = df_fcst.merge(X_test_full, how='right')\n",
					"\n",
					"    # drop rows where prediction or actuals are nan\n",
					"    # happens because of missing actuals\n",
					"    # or at edges of time due to lags/rolling windows\n",
					"    clean = together[together[[target_column_name,\n",
					"                               predicted_column_name]].notnull().all(axis=1)]\n",
					"    return(clean)\n",
					"\n",
					"\n",
					"df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)\n",
					"\n",
					"from azureml.automl.core._vendor.automl.client.core.common import metrics\n",
					"from matplotlib import pyplot as plt\n",
					"from automl.client.core.common import constants\n",
					"\n",
					"# use automl metrics module\n",
					"scores = metrics.compute_metrics_regression(\n",
					"    df_all['predicted'],\n",
					"    df_all[target_column_name],\n",
					"    list(constants.Metric.SCALAR_REGRESSION_SET),\n",
					"    None, None, None)\n",
					"\n",
					"print(\"[Test data scores]\\n\")\n",
					"for key, value in scores.items():    \n",
					"    print('{}:   {:.3f}'.format(key, value))\n",
					"    \n",
					"# Plot outputs\n",
					"#%matplotlib inline\n",
					"test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
					"test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
					"plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
					"plt.show()"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Closing\n",
					"\n",
					"At this point you should have a chart like this one in the image below, created wit AutoML and MatplotLib. The results are that good because of the **logQuantity** column, a  data Leakage calculated from que **quantity** column. You can try to run the same experiment without it.\n",
					"\n",
					"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/prediction.PNG\" alt=\"Chart\" width=\"75%\"/>\n",
					"\n",
					"## Next Steps\n",
					"\n",
					"1. [Deploy](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where) your Model.\n",
					"1. [Collect and Evaluate](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-enable-data-collection) Model Data.\n",
					"1. [Create](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline) a ML Pipeline.\n",
					"1. [Create](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-event-grid) Events Driven CI/CD Workflows.\n",
					"\n",
					""
				],
				"attachments": null
			}
		]
	}
}